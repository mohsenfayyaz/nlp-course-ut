{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_CA1_MohsenFayyaz.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 (Byte Pair Encoding (BPE))"
      ],
      "metadata": {
        "id": "uE7TjESzzFJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "corpus = flatten([[\"low\"] * 5, [\"lower\"] * 2, [\"widest\"] * 3, [\"newest\"] * 5])\n",
        "corpus = [list(s) for s in corpus]\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLmiGa_izGms",
        "outputId": "f82fbd68-e854-4951-806d-dfcf9db1641d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w', 'e', 'r'], ['l', 'o', 'w', 'e', 'r'], ['w', 'i', 'd', 'e', 's', 't'], ['w', 'i', 'd', 'e', 's', 't'], ['w', 'i', 'd', 'e', 's', 't'], ['n', 'e', 'w', 'e', 's', 't'], ['n', 'e', 'w', 'e', 's', 't'], ['n', 'e', 'w', 'e', 's', 't'], ['n', 'e', 'w', 'e', 's', 't'], ['n', 'e', 'w', 'e', 's', 't']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer_BPE:\n",
        "    def __init__(self, verbose=True):\n",
        "        self.vocab = None\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __printv(self, string):\n",
        "        if self.verbose:\n",
        "            print(string)\n",
        "\n",
        "    def __find_max_key(self, d: dict) -> str:\n",
        "        max_value = -1\n",
        "        for key, value in d.items():\n",
        "            if value > max_value:\n",
        "                max_key, max_value = key, value\n",
        "        return max_key\n",
        "\n",
        "    def __find_max_pair(self, corpus) -> str:\n",
        "        pair_freq = dict()\n",
        "        for word in corpus:\n",
        "            for char_idx in range(len(word) - 1):\n",
        "                pair = \"\".join(word[char_idx: char_idx + 2])\n",
        "                pair_freq[pair] = pair_freq.get(pair, 0) + 1\n",
        "        max_freq_pair = self.__find_max_key(pair_freq)\n",
        "        self.__printv(f\"{max_freq_pair} --> {pair_freq}\")\n",
        "        return max_freq_pair\n",
        "\n",
        "    def __merge_pair(self, corpus, pair: str):\n",
        "        for word_idx, word in enumerate(corpus):\n",
        "            merged_word = []\n",
        "            char_idx = 0\n",
        "            while char_idx < len(word):\n",
        "                if char_idx == len(word) - 1:\n",
        "                    merged_word.append(word[char_idx])\n",
        "                    break\n",
        "                current_pair = \"\".join(word[char_idx: char_idx + 2])\n",
        "                if current_pair == pair:\n",
        "                    merged_word.append(pair)\n",
        "                    char_idx += 2\n",
        "                else:\n",
        "                    merged_word.append(word[char_idx])\n",
        "                    char_idx += 1\n",
        "            corpus[word_idx] = merged_word\n",
        "        return corpus\n",
        "\n",
        "    def __get_unique_chars(self, corpus):\n",
        "        return list(set(flatten(corpus)))\n",
        "\n",
        "    def train(self, corpus: list, vocab_size: int) -> list:\n",
        "        output = {\"vocab\": self.__get_unique_chars(corpus)}\n",
        "        while len(output[\"vocab\"]) < vocab_size:\n",
        "            self.__printv(corpus)\n",
        "            try:\n",
        "                max_freq_pair = self.__find_max_pair(corpus)\n",
        "                corpus = self.__merge_pair(corpus, max_freq_pair)\n",
        "                output[\"vocab\"].append(max_freq_pair)\n",
        "                # self.__printv(\"\")\n",
        "            except Exception as e:\n",
        "                break\n",
        "        output[\"vocab\"] = output[\"vocab\"][:vocab_size]\n",
        "        self.vocab = output[\"vocab\"]\n",
        "        return output\n",
        "    \n",
        "    def tokenize(self, word: str):\n",
        "        word = list(word)\n",
        "        for merge_pair in self.vocab:\n",
        "            before_merge_len = len(word)\n",
        "            word = self.__merge_pair([word], merge_pair)[0]\n",
        "            if before_merge_len != len(word):\n",
        "                self.__printv(f\"Merged '{merge_pair}' --> {word}\")\n",
        "        return word"
      ],
      "metadata": {
        "id": "G20zha9SBcCI"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer = Tokenizer_BPE(verbose=True)\n",
        "print(\"### Training ###\")\n",
        "output = bpe_tokenizer.train(corpus.copy(), vocab_size=100)\n",
        "print(output)\n",
        "print(\"### Tokenizing ###\")\n",
        "print(bpe_tokenizer.tokenize(\"lowest\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaT5lT0JlL3p",
        "outputId": "0beecaaf-4a47-48e7-d26e-b0724e4774bd"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Training ###\n",
            "[['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w', 'e', 'r'], ['l', 'o', 'w', 'e', 'r'], ['w', 'i', 'd', 'e', 's', 't'], ['w', 'i', 'd', 'e', 's', 't'], ['w', 'i', 'd', 'e', 's', 't'], ['n', 'e', 'w', 'e', 's', 't'], ['n', 'e', 'w', 'e', 's', 't'], ['n', 'e', 'w', 'e', 's', 't'], ['n', 'e', 'w', 'e', 's', 't'], ['n', 'e', 'w', 'e', 's', 't']]\n",
            "es --> {'lo': 7, 'ow': 7, 'we': 7, 'er': 2, 'wi': 3, 'id': 3, 'de': 3, 'es': 8, 'st': 8, 'ne': 5, 'ew': 5}\n",
            "[['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w', 'e', 'r'], ['l', 'o', 'w', 'e', 'r'], ['w', 'i', 'd', 'es', 't'], ['w', 'i', 'd', 'es', 't'], ['w', 'i', 'd', 'es', 't'], ['n', 'e', 'w', 'es', 't'], ['n', 'e', 'w', 'es', 't'], ['n', 'e', 'w', 'es', 't'], ['n', 'e', 'w', 'es', 't'], ['n', 'e', 'w', 'es', 't']]\n",
            "est --> {'lo': 7, 'ow': 7, 'we': 2, 'er': 2, 'wi': 3, 'id': 3, 'des': 3, 'est': 8, 'ne': 5, 'ew': 5, 'wes': 5}\n",
            "[['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w'], ['l', 'o', 'w', 'e', 'r'], ['l', 'o', 'w', 'e', 'r'], ['w', 'i', 'd', 'est'], ['w', 'i', 'd', 'est'], ['w', 'i', 'd', 'est'], ['n', 'e', 'w', 'est'], ['n', 'e', 'w', 'est'], ['n', 'e', 'w', 'est'], ['n', 'e', 'w', 'est'], ['n', 'e', 'w', 'est']]\n",
            "lo --> {'lo': 7, 'ow': 7, 'we': 2, 'er': 2, 'wi': 3, 'id': 3, 'dest': 3, 'ne': 5, 'ew': 5, 'west': 5}\n",
            "[['lo', 'w'], ['lo', 'w'], ['lo', 'w'], ['lo', 'w'], ['lo', 'w'], ['lo', 'w', 'e', 'r'], ['lo', 'w', 'e', 'r'], ['w', 'i', 'd', 'est'], ['w', 'i', 'd', 'est'], ['w', 'i', 'd', 'est'], ['n', 'e', 'w', 'est'], ['n', 'e', 'w', 'est'], ['n', 'e', 'w', 'est'], ['n', 'e', 'w', 'est'], ['n', 'e', 'w', 'est']]\n",
            "low --> {'low': 7, 'we': 2, 'er': 2, 'wi': 3, 'id': 3, 'dest': 3, 'ne': 5, 'ew': 5, 'west': 5}\n",
            "[['low'], ['low'], ['low'], ['low'], ['low'], ['low', 'e', 'r'], ['low', 'e', 'r'], ['w', 'i', 'd', 'est'], ['w', 'i', 'd', 'est'], ['w', 'i', 'd', 'est'], ['n', 'e', 'w', 'est'], ['n', 'e', 'w', 'est'], ['n', 'e', 'w', 'est'], ['n', 'e', 'w', 'est'], ['n', 'e', 'w', 'est']]\n",
            "ne --> {'lowe': 2, 'er': 2, 'wi': 3, 'id': 3, 'dest': 3, 'ne': 5, 'ew': 5, 'west': 5}\n",
            "[['low'], ['low'], ['low'], ['low'], ['low'], ['low', 'e', 'r'], ['low', 'e', 'r'], ['w', 'i', 'd', 'est'], ['w', 'i', 'd', 'est'], ['w', 'i', 'd', 'est'], ['ne', 'w', 'est'], ['ne', 'w', 'est'], ['ne', 'w', 'est'], ['ne', 'w', 'est'], ['ne', 'w', 'est']]\n",
            "new --> {'lowe': 2, 'er': 2, 'wi': 3, 'id': 3, 'dest': 3, 'new': 5, 'west': 5}\n",
            "[['low'], ['low'], ['low'], ['low'], ['low'], ['low', 'e', 'r'], ['low', 'e', 'r'], ['w', 'i', 'd', 'est'], ['w', 'i', 'd', 'est'], ['w', 'i', 'd', 'est'], ['new', 'est'], ['new', 'est'], ['new', 'est'], ['new', 'est'], ['new', 'est']]\n",
            "newest --> {'lowe': 2, 'er': 2, 'wi': 3, 'id': 3, 'dest': 3, 'newest': 5}\n",
            "[['low'], ['low'], ['low'], ['low'], ['low'], ['low', 'e', 'r'], ['low', 'e', 'r'], ['w', 'i', 'd', 'est'], ['w', 'i', 'd', 'est'], ['w', 'i', 'd', 'est'], ['newest'], ['newest'], ['newest'], ['newest'], ['newest']]\n",
            "wi --> {'lowe': 2, 'er': 2, 'wi': 3, 'id': 3, 'dest': 3}\n",
            "[['low'], ['low'], ['low'], ['low'], ['low'], ['low', 'e', 'r'], ['low', 'e', 'r'], ['wi', 'd', 'est'], ['wi', 'd', 'est'], ['wi', 'd', 'est'], ['newest'], ['newest'], ['newest'], ['newest'], ['newest']]\n",
            "wid --> {'lowe': 2, 'er': 2, 'wid': 3, 'dest': 3}\n",
            "[['low'], ['low'], ['low'], ['low'], ['low'], ['low', 'e', 'r'], ['low', 'e', 'r'], ['wid', 'est'], ['wid', 'est'], ['wid', 'est'], ['newest'], ['newest'], ['newest'], ['newest'], ['newest']]\n",
            "widest --> {'lowe': 2, 'er': 2, 'widest': 3}\n",
            "[['low'], ['low'], ['low'], ['low'], ['low'], ['low', 'e', 'r'], ['low', 'e', 'r'], ['widest'], ['widest'], ['widest'], ['newest'], ['newest'], ['newest'], ['newest'], ['newest']]\n",
            "lowe --> {'lowe': 2, 'er': 2}\n",
            "[['low'], ['low'], ['low'], ['low'], ['low'], ['lowe', 'r'], ['lowe', 'r'], ['widest'], ['widest'], ['widest'], ['newest'], ['newest'], ['newest'], ['newest'], ['newest']]\n",
            "lower --> {'lower': 2}\n",
            "[['low'], ['low'], ['low'], ['low'], ['low'], ['lower'], ['lower'], ['widest'], ['widest'], ['widest'], ['newest'], ['newest'], ['newest'], ['newest'], ['newest']]\n",
            "{'vocab': ['d', 'o', 'l', 'w', 'n', 'i', 's', 't', 'r', 'e', 'es', 'est', 'lo', 'low', 'ne', 'new', 'newest', 'wi', 'wid', 'widest', 'lowe', 'lower']}\n",
            "### Tokenizing ###\n",
            "Merged 'es' --> ['l', 'o', 'w', 'es', 't']\n",
            "Merged 'est' --> ['l', 'o', 'w', 'est']\n",
            "Merged 'lo' --> ['lo', 'w', 'est']\n",
            "Merged 'low' --> ['low', 'est']\n",
            "['low', 'est']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 (HuggingFace Tokenizer)"
      ],
      "metadata": {
        "id": "bnInlsPBHxtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tokenizers\n",
        "! wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
        "! wget http://www.gutenberg.org/cache/epub/16457/pg16457.txt\n",
        "! unzip wikitext-103-raw-v1.zip"
      ],
      "metadata": {
        "id": "yYPViDAYH02m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE, WordPiece\n",
        "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "# files = [\"pg16457.txt\"]\n",
        "\n",
        "# BPE\n",
        "tokenizer_bpe = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=3*10**6)\n",
        "tokenizer_bpe.pre_tokenizer = Whitespace()  # Avoid \"it is\" as a token\n",
        "tokenizer_bpe.train(files, trainer)\n",
        "\n",
        "# WordPiece\n",
        "tokenizer_wp = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=3*10**6)\n",
        "tokenizer_wp.pre_tokenizer = Whitespace()  # Avoid \"it is\" as a token\n",
        "tokenizer_wp.train(files, trainer)"
      ],
      "metadata": {
        "id": "WtGY1mLjIQ2k"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer_bpe.get_vocab_size(), tokenizer_bpe.get_vocab())\n",
        "print(tokenizer_wp.get_vocab_size(), tokenizer_wp.get_vocab())"
      ],
      "metadata": {
        "id": "7O216P2qJOc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer_bpe.get_vocab_size())\n",
        "print(tokenizer_wp.get_vocab_size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkJL7zKUwJ2J",
        "outputId": "35f635e0-b73e-4e86-b111-6362d515e339"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "777366\n",
            "812990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer_bpe.encode(\"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!😁 ?\")\n",
        "print(\"BPE\", len(output.tokens), output.tokens)\n",
        "output = tokenizer_wp.encode(\"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!😁 ?\")\n",
        "print(\"WP \", len(output.tokens), output.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpYTpJ5tJxpQ",
        "outputId": "cd82015f-6aaa-40ee-f0d7-8534291b0b49"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BPE 42 ['This', 'is', 'a', 'deep', 'learning', 'token', 'ization', 'tutorial', '.', 'Token', 'ization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'NL', 'P', 'pipeline', '.', 'We', 'will', 'be', 'comparing', 'the', 'tokens', 'generated', 'by', 'each', 'token', 'ization', 'model', '.', 'Excited', 'much', '?', '!', '[UNK]', '?']\n",
            "WP  40 ['This', 'is', 'a', 'deep', 'learning', 'token', '##ization', 'tutorial', '.', 'Token', '##ization', 'is', 'the', 'first', 'step', 'in', 'a', 'deep', 'learning', 'NL', '##P', 'pipeline', '.', 'We', 'will', 'be', 'comparing', 'the', 'tokens', 'generated', 'by', 'each', 'token', '##ization', 'model', '.', 'Excited', 'much', '[UNK]', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"pg16457.txt\", \"r\") as f:\n",
        "    g = f.read()\n",
        "\n",
        "output = tokenizer_bpe.encode(g)\n",
        "print(\"BPE\", len(output.tokens))\n",
        "output = tokenizer_wp.encode(g)\n",
        "print(\"WP \", len(output.tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_7AwZvZ4CHc",
        "outputId": "6aaa55cd-45a0-4b6f-a9c7-089eb5414325"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BPE 127262\n",
            "WP  124054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EdGQe3SC7PXH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}